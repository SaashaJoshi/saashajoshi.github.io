[{"content":"\u003cp\u003eHello readers,\u003c/p\u003e\n\u003cp\u003eA couple of weeks ago I defended my Master\u0026rsquo;s thesis titled \u003cem\u003epiQture: A Quantum Machine Learning (QML) Library for Image Processing\u003c/em\u003e\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. Encouraged by the response and interest in \u003cem\u003epiQture\u003c/em\u003e, I am here with a series of quick-start guides to the library.\u003c/p\u003e\n\u003ch2 id=\"introducing-piqture\"\u003eIntroducing \u003cem\u003epiQture\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003epiQture\u003c/em\u003e is a Python and Qiskit-based software framework designed to accommodate users familiar with classical machine learning but without prior experience in QML. It provides users with an accessible workflow to design, implement, and experiment with QML models for real-life applications such as image processing.\u003c/p\u003e\n\u003ch2 id=\"quantum-image-representation-qir\"\u003eQuantum Image Representation (QIR)\u003c/h2\u003e\n\u003cp\u003eToday, I wish to dive into one of \u003cem\u003epiQture\u003c/em\u003e\u0026rsquo;s standout features — implementing Quantum Image Representation (QIR) methods.\u003c/p\u003e\n\u003cp\u003eQIR is a data embedding technique that provides an interface between classical and quantum platforms for representing digital images on quantum devices. Digital images typically have attributes such as pixel position and color information that can be encoded onto a quantum circuit using specific unitary transforms.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content/images/2x2 image.png\"\u003e\n    \u003cfigcaption\u003eA 2x2 image with pixel positions (00, 01, 10, 11) and their corresponding color information (5, 50, 150, 255)\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003ch3 id=\"understanding-ineqr\"\u003eUnderstanding INEQR\u003c/h3\u003e\n\u003cp\u003eWhile many QIR techniques exist, this article focuses on the \u003cstrong\u003eImproved Novel Enhanced Quantum Representation (INEQR)\u003c/strong\u003e method for image representation.\u003c/p\u003e\n\u003cp\u003eINEQR, introduced by Nan Jiang and Luo Wang\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e, supports encoding non-square images with unequal horizontal (X) and vertical (Y) dimensions onto a quantum circuit.\u003c/p\u003e\n\u003cp\u003eINEQR employs unitary operations like Hadamard (H) and Controlled-NOT (CX) to capture the pixel position and color information, respectively. It utilizes the basis states of the qubits to represent this information, resulting in a deterministic image retrieval process. A significant limitation of INEQR is its inability to encode colored images.\u003c/p\u003e\n\u003ch3 id=\"a-little-math-perhaps-else-skip-to-the-implementation-optional\"\u003eA Little Math, Perhaps? Else, Skip to the Implementation! (Optional)\u003c/h3\u003e\n\u003cp\u003eINEQR employs two unitary transforms:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eHadamard\u003c/em\u003e transform for encoding the pixel position.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eMulti-CX\u003c/em\u003e transform for encoding the color information.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFor simplicity, let us consider a grayscale image of size 2x2, with gray values in the range [0, 255].\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 1:\u003c/strong\u003e\u003cbr\u003e\nThe pixel positions of the four pixels in a 2x2 image can be represented in their binary formats as 00, 01, 10, and 11, which correspond to the coordinates of each pixel in the image grid:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTop-left pixel corresponds to (Y = 0, X = 0)\u003c/li\u003e\n\u003cli\u003eTop-right pixel corresponds to (Y = 0, X = 1)\u003c/li\u003e\n\u003cli\u003eBottom-left pixel corresponds to (Y = 1, X = 0)\u003c/li\u003e\n\u003cli\u003eBottom-left pixel corresponds to (Y = 1, X = 1)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eINEQR utilizes a \u003cem\u003eHadamard\u003c/em\u003e transform that encodes these pixel positions on √4 = 2 qubits (q₀ and q₁).\u003c/p\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/hadamard-trans.png\"\u003e\n    \u003cfigcaption\u003eStep 1: A Hadamard transform that uses 2 qubits to encode four pixel positions.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/hadamard-circ.png\"\u003e\n    \u003cfigcaption\u003eHadamard transform encodes pixel position on two qubits.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003cp\u003eDuring practical implementation, Hadamard gates are followed by X gates, preparing an oracle-like structure for the multi-CX gates to encode color information corresponding to each pixel position.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStep 2:\u003c/strong\u003e\u003cbr\u003e\nThe first action in this step is to convert the gray values in the range [0, 255] to their corresponding binary formats. For example, 50 in binary can be given as 00110010, and 255 as 11111111.\u003c/p\u003e\n\u003cp\u003eNow, a \u003cem\u003eMulti-CX\u003c/em\u003e transform encodes the binary color information onto an additional 8 qubits (q₂ to q₉). For a 2x2 image, a CCX unitary gate, with controls on the first two positional qubits (q₀ and q₁), is applied to qubit qᵢ when the i-th color bit is 1.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/cx-trans.png\"\u003e\n    \u003cfigcaption\u003eStep 2: A CX transform that utilizes 8 qubits to encode a color value in the range [0, 255]. The color value is represented in its binary format (shown in figure, binary of color value 50).\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/cx-circ.png\"\u003e\n    \u003cfigcaption\u003eCCX operations encode the color information. This image shows the encoding of the color value 49.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003cp\u003eCombining the transforms in Step 1 and Step 2, an INEQR encoded 2x2 image can be given as:\u003c/p\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/final-ineqr.png\"\u003e\n    \u003cfigcaption\u003eAn INEQR representation for a 2x2 image\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003cp\u003eThe total qubit requirement for INEQR is \u003cstrong\u003en₁ + n₂ + q\u003c/strong\u003e. Remember, a generalized INEQR method can encode non-square images.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/general-ineqr.png\"\u003e\n    \u003cfigcaption\u003eAn INEQR representation for any non-square image\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003ch2 id=\"implementation-with-piqture\"\u003eImplementation with \u003cem\u003epiQture\u003c/em\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003epiQture\u003c/em\u003e has an in-built implementation for \u003ccode\u003eINEQR\u003c/code\u003e in the \u003ccode\u003eimage_representations\u003c/code\u003e module. Let us see how \u003cem\u003epiQture\u003c/em\u003e can be utilized to build an INEQR embedding circuit.\u003c/p\u003e\n\u003cp\u003eNote: This implementation utilizes the \u003ccode\u003eload_mnist_dataset\u003c/code\u003e function from the \u003ccode\u003edata_loader\u003c/code\u003e module in \u003cem\u003epiQture\u003c/em\u003e to import an MNIST dataset from PyTorch databases.\u003c/p\u003e\n\u003cp\u003eAlright, let us start by performing some imports.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e torch\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e piqture.data_loader.mnist_data_loader \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e load_mnist_dataset\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003efrom\u003c/span\u003e piqture.embeddings.image_embeddings.ineqr \u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e INEQR\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eNext, load the MNIST dataset using the \u003ccode\u003eload_mnist_dataset\u003c/code\u003e function.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Resize images to 2x2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eimg_size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e2\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003etrain_dataset, test_dataset \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e load_mnist_dataset(img_size)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Retrieve a single image from the dataset\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eimage, label \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e train_dataset[\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eimage_size \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e tuple(image\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esqueeze()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esize())\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBy default, the MNIST images are of the type \u003ccode\u003etensor\u003c/code\u003e with \u003ccode\u003efloat\u003c/code\u003e values. We transform these color values into \u003ccode\u003eintegers\u003c/code\u003e and further to their \u003ccode\u003ebinary\u003c/code\u003e representations.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# Change pixel values from tensor to list\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epixel_vals \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (image \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e255\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eround()\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eto(torch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003euint8)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epixel_vals \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pixel_vals\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003etolist()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprint(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Label: \u0026#34;\u003c/span\u003e, label, \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003ePixel values: \u0026#34;\u003c/span\u003e, pixel_vals)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eFinally, let us generate the INEQR circuit with \u003cem\u003epiQture\u003c/em\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eembedding \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e INEQR(image_size, pixel_vals)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eineqr()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eembedding\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edraw(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;mpl\u0026#34;\u003c/span\u003e, style\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;iqp\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ccenter\u003e\n\u003cfigure\u003e\n    \u003cimg src=\"https://github.com/SaashaJoshi/saashajoshi.github.io/tree/main/content//images/ineqr-circ.png\"\u003e\n    \u003cfigcaption\u003eAn INEQR circuit for a 2x2 grayscale MNIST image with color information [[38, 49], [46, 41]]\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003c/center\u003e\n\u003cp\u003eThat is all for today. Stay tuned to the \u003ca href=\"https://github.com/SaashaJoshi/piQture\"\u003epiQture\u003c/a\u003e repository for more intriguing implementations and the \u003ca href=\"https://github.com/SaashaJoshi/piQture-demos\"\u003epiQture-demos\u003c/a\u003e repository for upcoming demos and tutorials on QIR.\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eS. Joshi, “piQture: A Quantum Machine Learning Library for Image Processing,” dspace.library.uvic.ca, 2024, Accessed: Jun. 24, 2024. [Online]. Available: \u003ca href=\"https://dspace.library.uvic.ca/items/a21a2dca-f0c3-465d-b1c7-40b122b67697\"\u003ehttps://dspace.library.uvic.ca/items/a21a2dca-f0c3-465d-b1c7-40b122b67697\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eNan Jiang and Luo Wang. Quantum image scaling using nearest neighbor interpolation. Quantum Information Processing, 14(5):1559–1571, 2015.\u0026#160;\u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n","description":"An introduction to Quantum Image Representations (QIR).","image":null,"permalink":"http://localhost:1313/blogs/quick-start-guide-quantum-image-representation-with-piqture/","title":"Quick-Start Guide: Quantum Image Representation with piQture"},{"content":"\u003ch1 id=\"quantum-machine-learning-with-circuit-cutting\"\u003eQuantum Machine Learning with Circuit Cutting\u003c/h1\u003e\n\u003cp\u003eQuantum Machine Learning (QML) techniques, including variational Quantum Tensor Networks (QTN), pose a huge implementation challenge regarding qubit requirements. An approach to circumvent this issue is to perform circuit cutting that segments large quantum circuits into multiple smaller sub-circuits that can be trained easily on a quantum device \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e. This project lays down the workflow for training a variational QTN circuit that implements circuit cutting, specifically gate cutting, to perform data classification. The workflow is built with the help of the Qiskit SDK with dependence on the Circuit Knitting Toolbox \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e for circuit cutting procedures. Additionally, a significant amount of modifications are made to algorithms like SamplerQNN, a part of the Qiskit Machine Learning package, to accommodate the training of multiple sub-circuits with Qiskit Aer\u0026rsquo;s Sampler runtime primitive. The dataset used for classification is the diabetes dataset from the National Institute of Diabetes and Digestive and Kidney Diseases publicly available on Kaggle.\u003c/p\u003e\n\u003ch2 id=\"workflow\"\u003eWorkflow\u003c/h2\u003e\n\u003cp\u003eThe training of QML models integrated with circuit cutting can be performed using two possible workflows.\u003c/p\u003e\n\u003cp\u003eIn workflow A, sub-circuits generated after a circuit cutting undergo training using a subset of input data features. Evaluation is subsequently performed with respect to the original training labels. Following this, the circuits undergo a tuning stage, wherein variable parameters are updated based on the loss function computed in the evaluation stage. Upon obtaining optimal parameter values from the optimizer, the quasi-probability distributions derived from the sub-circuits are combined to reconstruct the expectation value of the original circuit.\u003c/p\u003e\n\u003cp\u003eThis workflow facilitates an independent and parallel evaluation of sub-circuits over multiple iterations. The concurrent training can be performed with the help of existing Batching techniques in the Qiskit Runtime primitives \u003csup id=\"fnref1:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/Cut-QTN/blob/main/graphics/Workflow-A.png\" alt=\"Training Workflow for QML model with Circuit Cutting\"\u003e\n\u003cp\u003eWorkflow B, also proposed in \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e, involves training the sub-circuits to reconstruct the original expectation values after each training iteration. This is unlike Workflow A, where the reconstruction stage occurs after multiple iterations are performed on each sub-circuit. Subsequently, an optimization step is performed on the reconstructed expectation value to tune the variable parameters within the sub-circuits. The updated parameters are then utilized to finalize the training process. The ultimately reconstructed expectation value is further used for validation and testing purposes.\u003c/p\u003e\n\u003cp\u003eThis workflow facilitates concurrent training over one training iteration at a time. This is unlike Workflow A that allows for the parallel execution of sub-circuits over multiple iterations. Following the completion of each training iteration, the sub-circuits undergo classical post-processing steps to reconstruct the original expectation value.\u003c/p\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/Cut-QTN/blob/main/graphics/Workflow-B.png\" alt=\"Training Workflow for QML model with Circuit Cutting\"\u003e\n\u003cp\u003eFor this project, we have opted to adopt the Workflow A structure. This design decision aligns with considerations related to the constraints imposed by the current Qiskit stack, which currently lacks support for training sub-circuits using the existing SamplerQNN primitive. Additionally, this decision is influenced by the time limitations inherent in the project timeline.\u003c/p\u003e\n\u003ch2 id=\"evaluation-and-results\"\u003eEvaluation and Results\u003c/h2\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/all_forward_time.png\" alt=\"Time Taken to Run One Forward Pass on Different Backends.\"\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/all_backward_time.png\" alt=\"Time Taken to Run One Backward Pass on Different Backends.\"\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/final_results_table.png\" alt=\"Results from Training an 8-qubit QML Model with 1 Circuit Cut on CPU and GPU\"\u003e\n\u003cdiv class=\"image-container\"\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/50-iter-cpu/sub-A.png\" alt=\"Training Loss in 4-qubit sub-circuits (A) on a CPU (50 iterations)\"\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/50-iter-cpu/sub-B.png\" alt=\"Training Loss in 4-qubit sub-circuits (B) on a CPU (50 iterations)\"\u003e\n\u003c/div\u003e\n\u003cdiv class=\"image-container\"\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/50-iter-gpu/sub-A.png\" alt=\"Training Loss in 4-qubit sub-circuits (A) on a GPU (50 iterations)\"\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/50-iter-gpu/sub-B.png\" alt=\"Training Loss in 4-qubit sub-circuits (B) on a GPU (50 iterations)\"\u003e\n\u003c/div\u003e\n\u003cimg src=\"https://github.com/SaashaJoshi/QML-circuit-cutting/blob/main/graphics/CC_time_to_train.png\" alt=\"Time to Train the QML Model with Circuit Cutting on CPU and GPU\"\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eD. Guala, S. Zhang, E. Cruz, C. A. Riofrío, J. Klepsch, and J. M. Arrazola, “Practical overview of image classification with tensor-network quantum circuits,” Scientific Reports, vol. 13, no. 1, p. 4427, Mar. 2023, doi: \u003ca href=\"https://doi.org/10.1038/s41598-023-30258-y\"\u003ehttps://doi.org/10.1038/s41598-023-30258-y\u003c/a\u003e.\u0026#160;\u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eJim Garrison, ‘Qiskit-Extensions/circuit-knitting-toolbox: Circuit Knitting Toolbox 0.6.0’. Zenodo, Feb. 12, 2024. doi: 10.5281/zenodo.10651875.\u0026#160;\u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u0026#160;\u003ca href=\"#fnref1:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eM. Beisel, J. Barzen, M. Bechtold, F. Leymann, F. Truger, and B. Weder, “QuantME4VQA: Modeling and Executing Variational Quantum Algorithms Using Workflows,” Proceedings of the 13th International Conference on Cloud Computing and Services Science, 2023, doi: \u003ca href=\"https://doi.org/10.5220/0011997500003488\"\u003ehttps://doi.org/10.5220/0011997500003488\u003c/a\u003e.\u0026#160;\u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e\u0026#x21a9;\u0026#xfe0e;\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/blogs/sample/","title":""}]